{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/local/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /usr/local/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "import re\n",
    "import xml.sax.saxutils as saxutils\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, LSTM\n",
    "from keras import utils, callbacks,layers, models \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import codecs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "#General constants (modify them according to you environment)\n",
    "\n",
    "# Set Numpy random seed\n",
    "random.seed(1000)\n",
    "\n",
    "# Newsline folder and format\n",
    "data_folder = '/Users/xiuli/Downloads/reuters21578/' \n",
    "\n",
    "sgml_number_of_files = 22\n",
    "sgml_file_name_template = 'reut2-NNN.sgm'\n",
    "\n",
    "# Category files\n",
    "category_files = 'all-topics-strings.lc.txt'\n",
    "\n",
    "# Word2Vec number of features\n",
    "num_features = 500\n",
    "# Limit each newsline to a fixed number of words\n",
    "document_max_num_words = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All categories within Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all categories\n",
    "category_data = []\n",
    "\n",
    "with open(data_folder + category_files, 'r') as file:\n",
    "    for category in file.readlines():\n",
    "        category_data.append([category.strip().lower()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['acq'],\n",
       " ['alum'],\n",
       " ['austdlr'],\n",
       " ['austral'],\n",
       " ['barley'],\n",
       " ['bfr'],\n",
       " ['bop'],\n",
       " ['can'],\n",
       " ['carcass'],\n",
       " ['castor-meal'],\n",
       " ['castor-oil'],\n",
       " ['castorseed'],\n",
       " ['citruspulp'],\n",
       " ['cocoa'],\n",
       " ['coconut'],\n",
       " ['coconut-oil'],\n",
       " ['coffee'],\n",
       " ['copper'],\n",
       " ['copra-cake'],\n",
       " ['corn'],\n",
       " ['corn-oil'],\n",
       " ['cornglutenfeed'],\n",
       " ['cotton'],\n",
       " ['cotton-meal'],\n",
       " ['cotton-oil'],\n",
       " ['cottonseed'],\n",
       " ['cpi'],\n",
       " ['cpu'],\n",
       " ['crude'],\n",
       " ['cruzado'],\n",
       " ['dfl'],\n",
       " ['dkr'],\n",
       " ['dlr'],\n",
       " ['dmk'],\n",
       " ['drachma'],\n",
       " ['earn'],\n",
       " ['escudo'],\n",
       " ['f-cattle'],\n",
       " ['ffr'],\n",
       " ['fishmeal'],\n",
       " ['flaxseed'],\n",
       " ['fuel'],\n",
       " ['gas'],\n",
       " ['gnp'],\n",
       " ['gold'],\n",
       " ['grain'],\n",
       " ['groundnut'],\n",
       " ['groundnut-meal'],\n",
       " ['groundnut-oil'],\n",
       " ['heat'],\n",
       " ['hk'],\n",
       " ['hog'],\n",
       " ['housing'],\n",
       " ['income'],\n",
       " ['instal-debt'],\n",
       " ['interest'],\n",
       " ['inventories'],\n",
       " ['ipi'],\n",
       " ['iron-steel'],\n",
       " ['jet'],\n",
       " ['jobs'],\n",
       " ['l-cattle'],\n",
       " ['lead'],\n",
       " ['lei'],\n",
       " ['lin-meal'],\n",
       " ['lin-oil'],\n",
       " ['linseed'],\n",
       " ['lit'],\n",
       " ['livestock'],\n",
       " ['lumber'],\n",
       " ['lupin'],\n",
       " ['meal-feed'],\n",
       " ['mexpeso'],\n",
       " ['money-fx'],\n",
       " ['money-supply'],\n",
       " ['naphtha'],\n",
       " ['nat-gas'],\n",
       " ['nickel'],\n",
       " ['nkr'],\n",
       " ['nzdlr'],\n",
       " ['oat'],\n",
       " ['oilseed'],\n",
       " ['orange'],\n",
       " ['palladium'],\n",
       " ['palm-meal'],\n",
       " ['palm-oil'],\n",
       " ['palmkernel'],\n",
       " ['peseta'],\n",
       " ['pet-chem'],\n",
       " ['platinum'],\n",
       " ['plywood'],\n",
       " ['pork-belly'],\n",
       " ['potato'],\n",
       " ['propane'],\n",
       " ['rand'],\n",
       " ['rape-meal'],\n",
       " ['rape-oil'],\n",
       " ['rapeseed'],\n",
       " ['red-bean'],\n",
       " ['reserves'],\n",
       " ['retail'],\n",
       " ['rice'],\n",
       " ['ringgit'],\n",
       " ['rubber'],\n",
       " ['rupiah'],\n",
       " ['rye'],\n",
       " ['saudriyal'],\n",
       " ['sfr'],\n",
       " ['ship'],\n",
       " ['silk'],\n",
       " ['silver'],\n",
       " ['singdlr'],\n",
       " ['skr'],\n",
       " ['sorghum'],\n",
       " ['soy-meal'],\n",
       " ['soy-oil'],\n",
       " ['soybean'],\n",
       " ['stg'],\n",
       " ['strategic-metal'],\n",
       " ['sugar'],\n",
       " ['sun-meal'],\n",
       " ['sun-oil'],\n",
       " ['sunseed'],\n",
       " ['tapioca'],\n",
       " ['tea'],\n",
       " ['tin'],\n",
       " ['trade'],\n",
       " ['tung'],\n",
       " ['tung-oil'],\n",
       " ['veg-oil'],\n",
       " ['wheat'],\n",
       " ['wool'],\n",
       " ['wpi'],\n",
       " ['yen'],\n",
       " ['zinc']]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_data #list, 135 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_tags(text):\n",
    "    return re.sub('<[^<]+?>', '', text).strip()\n",
    "\n",
    "def unescape(text):\n",
    "    return saxutils.unescape(text)\n",
    "#train_Ye=to_category_vector(train_Y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse SGML files and Iterate all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: reut2-000.sgm\n",
      "Reading file: reut2-001.sgm\n",
      "Reading file: reut2-002.sgm\n",
      "Reading file: reut2-003.sgm\n",
      "Reading file: reut2-004.sgm\n",
      "Reading file: reut2-005.sgm\n",
      "Reading file: reut2-006.sgm\n",
      "Reading file: reut2-007.sgm\n",
      "Reading file: reut2-008.sgm\n",
      "Reading file: reut2-009.sgm\n",
      "Reading file: reut2-010.sgm\n",
      "Reading file: reut2-011.sgm\n",
      "Reading file: reut2-012.sgm\n",
      "Reading file: reut2-013.sgm\n",
      "Reading file: reut2-014.sgm\n",
      "Reading file: reut2-015.sgm\n",
      "Reading file: reut2-016.sgm\n",
      "Reading file: reut2-017.sgm\n",
      "Reading file: reut2-018.sgm\n",
      "Reading file: reut2-019.sgm\n",
      "Reading file: reut2-020.sgm\n",
      "Reading file: reut2-021.sgm\n"
     ]
    }
   ],
   "source": [
    "document_X = {}\n",
    "document_Y = {}\n",
    "\n",
    "document_id=[]\n",
    "document_body=[]\n",
    "document_train_test=[]\n",
    "category=[]\n",
    "document_X=pd.DataFrame()\n",
    "\n",
    "for i in range(sgml_number_of_files):\n",
    "    if i < 10:\n",
    "        seq = '00' + str(i)\n",
    "    else:\n",
    "        seq = '0' + str(i)\n",
    "        \n",
    "    file_name = sgml_file_name_template.replace('NNN', seq)\n",
    "    print('Reading file: %s' % file_name)\n",
    "    \n",
    "    with codecs.open(data_folder + file_name, 'r', encoding='utf-8',\n",
    "                 errors='ignore') as file:\n",
    "        content = BeautifulSoup(file.read().lower())      \n",
    "        for newsline in content('reuters'):\n",
    "            for c in newsline.find(\"topics\").findAll('d'):\n",
    "                      \n",
    "            # News-line Id\n",
    "                document_id.append(newsline['newid'])\n",
    "            \n",
    "            # News-line text\n",
    "                document_body.append(unescape(strip_tags(str(newsline('text')[0].body)).replace('reuter\\n&#3;', '')))\n",
    "#            document_body.append(newsline.find(\"title\").text)\n",
    "                document_train_test.append(strip_tags(str(newsline.attrs[\"lewissplit\"]) ))\n",
    "                category.append(c.text)\n",
    "        \n",
    "\n",
    "            \n",
    "document_X['document_id'] = document_id\n",
    "document_X['document_body'] = document_body\n",
    "document_X['train_test_label'] = document_train_test\n",
    "document_X['category'] = category\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check statistics how many articles under each category\n",
    "#### could have filter categories that have few articles and strategy to balance the number of articles for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 document_id  document_body  train_test_label\n",
      "category                                                     \n",
      "acq                     2448           2448              2448\n",
      "alum                      63             63                63\n",
      "austdlr                    4              4                 4\n",
      "barley                    54             54                54\n",
      "bfr                        1              1                 1\n",
      "bop                      116            116               116\n",
      "can                        3              3                 3\n",
      "carcass                   75             75                75\n",
      "castor-oil                 2              2                 2\n",
      "castorseed                 1              1                 1\n",
      "citruspulp                 1              1                 1\n",
      "cocoa                     76             76                76\n",
      "coconut                    6              6                 6\n",
      "coconut-oil                7              7                 7\n",
      "coffee                   145            145               145\n",
      "copper                    78             78                78\n",
      "copra-cake                 3              3                 3\n",
      "corn                     254            254               254\n",
      "corn-oil                   1              1                 1\n",
      "cornglutenfeed             2              2                 2\n",
      "cotton                    63             63                63\n",
      "cotton-oil                 3              3                 3\n",
      "cottonseed                 1              1                 1\n",
      "cpi                      112            112               112\n",
      "cpu                        4              4                 4\n",
      "crude                    634            634               634\n",
      "cruzado                    1              1                 1\n",
      "dfl                        3              3                 3\n",
      "dkr                        1              1                 1\n",
      "dlr                      217            217               217\n",
      "...                      ...            ...               ...\n",
      "rice                      67             67                67\n",
      "ringgit                    1              1                 1\n",
      "rubber                    51             51                51\n",
      "rupiah                     1              1                 1\n",
      "rye                        2              2                 2\n",
      "saudriyal                  3              3                 3\n",
      "sfr                        3              3                 3\n",
      "ship                     305            305               305\n",
      "silver                    37             37                37\n",
      "skr                        1              1                 1\n",
      "sorghum                   35             35                35\n",
      "soy-meal                  27             27                27\n",
      "soy-oil                   25             25                25\n",
      "soybean                  120            120               120\n",
      "stg                       21             21                21\n",
      "strategic-metal           32             32                32\n",
      "sugar                    184            184               184\n",
      "sun-meal                   2              2                 2\n",
      "sun-oil                    8              8                 8\n",
      "sunseed                   17             17                17\n",
      "tapioca                    4              4                 4\n",
      "tea                       15             15                15\n",
      "tin                       33             33                33\n",
      "trade                    552            552               552\n",
      "veg-oil                  137            137               137\n",
      "wheat                    306            306               306\n",
      "wool                       2              2                 2\n",
      "wpi                       32             32                32\n",
      "yen                       69             69                69\n",
      "zinc                      44             44                44\n",
      "\n",
      "[120 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "ct=document_X.groupby('category').count() #120 categories under Topics in the dataset\n",
    "#Length: 14302\n",
    "\n",
    "#document_X1=document_X [document_X.category.isin(ct[ct.document_id>10].reset_index)]   #66 left\n",
    "print(ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleansing and train/test data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X=document_X[(document_X['train_test_label']=='train') & (document_X['category']!='')]['document_body']\n",
    "test_X=document_X[(document_X['train_test_label']=='test') & (document_X['category']!='')]['document_body']\n",
    "\n",
    "train_Y=document_X[(document_X['train_test_label']=='train') & (document_X['category']!='')]['category']\n",
    "test_Y=document_X[(document_X['train_test_label']=='test') & (document_X['category']!='')]['category']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stop-words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize tokenizer\n",
    "# It's also possible to try with a stemmer or to mix a stemmer and a lemmatizer\n",
    "tokenizer = RegexpTokenizer('[\\'a-zA-Z]+')\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Tokenized document collection\n",
    "train_Xt = []\n",
    "test_Xt = []\n",
    "def tokenize(document):\n",
    "    words = []\n",
    "\n",
    "    for sentence in sent_tokenize(document):\n",
    "        tokens = [lemmatizer.lemmatize(t.lower()) for t in tokenizer.tokenize(sentence) if t.lower() not in stop_words]\n",
    "        words += tokens\n",
    "\n",
    "    return words\n",
    "\n",
    "# Tokenize\n",
    "\n",
    "\n",
    "for index, row in train_X.items():\n",
    "    train_Xt.append(tokenize(row))\n",
    "\n",
    "    \n",
    "for index, row in test_X.items():\n",
    "    test_Xt.append(tokenize(row))\n",
    "    \n",
    "number_of_documents = 14302#len(document_X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create new Gensim Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(train_Xt, size=num_features, min_count=1, window=10, workers=cpu_count())\n",
    "w2v_model.init_sims(replace=True)\n",
    "w2v_model.save(data_folder + 'reuters.word2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  from ipykernel import kernelapp as app\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  app.launch_new_instance()\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:26: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "num_categories = 115\n",
    "train_Xe = zeros(shape=(number_of_documents, document_max_num_words, num_features)).astype(float32)\n",
    "train_ye = zeros(shape=(number_of_documents, num_categories)).astype(float32)\n",
    "test_Xe = zeros(shape=(number_of_documents, document_max_num_words, num_features)).astype(float32)\n",
    "test_ye = zeros(shape=(number_of_documents, num_categories)).astype(float32)\n",
    "\n",
    "empty_word = zeros(num_features).astype(float32)\n",
    "\n",
    "for idx, document in enumerate(train_Xt):\n",
    "    for jdx, word in enumerate(document):\n",
    "        if jdx == document_max_num_words:\n",
    "            break\n",
    "            \n",
    "        else:\n",
    "            if word in w2v_model:\n",
    "                train_Xe[idx, jdx, :] = w2v_model[word]\n",
    "            else:\n",
    "                train_Xe[idx, jdx, :] = empty_word\n",
    "\n",
    "for idx, document in enumerate(test_Xt):\n",
    "    for jdx, word in enumerate(document):\n",
    "        if jdx == document_max_num_words:\n",
    "            break\n",
    "            \n",
    "        else:\n",
    "            if word in w2v_model:\n",
    "                test_Xe[idx, jdx, :] = w2v_model[word]\n",
    "            else:\n",
    "                test_Xe[idx, jdx, :] = empty_word\n",
    "\n",
    "#train_Ye=to_category_vector(train_Y)\n",
    "#test_Ye=to_category_vector(test_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Ye=utils.to_categorical(pd.factorize(train_Y)[0])\n",
    "test_Ye=utils.to_categorical(pd.factorize(test_Y)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Keras model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(int(document_max_num_words*1.5), input_shape=(document_max_num_words, num_features)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_categories))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#Train and evaluate model\n",
    "\n",
    "# Train model\n",
    "model.fit(train_Xe, train_Ye, batch_size=128, nb_epoch=5, validation_data=(test_Xe, test_Ye))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "score, acc = model.evaluate(test_Xe, test_Ye, batch_size=128)\n",
    "    \n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
