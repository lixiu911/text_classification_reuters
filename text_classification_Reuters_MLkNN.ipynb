{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import xml.sax.saxutils as saxutils\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import codecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#General constants \n",
    "\n",
    "# Newsline folder and format\n",
    "data_folder = '/Users/xiuli/Downloads/reuters21578/' \n",
    "\n",
    "sgml_number_of_files = 22\n",
    "sgml_file_name_template = 'reut2-NNN.sgm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_tags(text):\n",
    "    return re.sub('<[^<]+?>', '', text).strip()\n",
    "\n",
    "def unescape(text):\n",
    "    return saxutils.unescape(text)\n",
    "#train_Ye=to_category_vector(train_Y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "def cleanHtml(sentence):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, ' ', str(sentence))\n",
    "    return cleantext\n",
    "def cleanPunc(sentence): #function to clean the word of any punctuation or special characters\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    cleaned = cleaned.strip()\n",
    "    cleaned = cleaned.replace(\"\\n\",\" \")\n",
    "    return cleaned\n",
    "def keepAlpha(sentence):\n",
    "    alpha_sent = \"\"\n",
    "    for word in sentence.split():\n",
    "        alpha_word = re.sub('[^a-z A-Z]+', ' ', word)\n",
    "        alpha_sent += alpha_word\n",
    "        alpha_sent += \" \"\n",
    "    alpha_sent = alpha_sent.strip()\n",
    "    return alpha_sent\n",
    "stop_words = set(stopwords.words('english'))\n",
    "#stop_words.update(['zero','one','two','three','four','five','six','seven','eight','nine','ten','may','also','across','among','beside','however','yet','within'])\n",
    "re_stop_words = re.compile(r\"\\b(\" + \"|\".join(stop_words) + \")\\\\W\", re.I)\n",
    "def removeStopWords(sentence):\n",
    "    global re_stop_words\n",
    "    return re_stop_words.sub(\" \", sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: reut2-000.sgm\n",
      "Reading file: reut2-001.sgm\n",
      "Reading file: reut2-002.sgm\n",
      "Reading file: reut2-003.sgm\n",
      "Reading file: reut2-004.sgm\n",
      "Reading file: reut2-005.sgm\n",
      "Reading file: reut2-006.sgm\n",
      "Reading file: reut2-007.sgm\n",
      "Reading file: reut2-008.sgm\n",
      "Reading file: reut2-009.sgm\n",
      "Reading file: reut2-010.sgm\n",
      "Reading file: reut2-011.sgm\n",
      "Reading file: reut2-012.sgm\n",
      "Reading file: reut2-013.sgm\n",
      "Reading file: reut2-014.sgm\n",
      "Reading file: reut2-015.sgm\n",
      "Reading file: reut2-016.sgm\n",
      "Reading file: reut2-017.sgm\n",
      "Reading file: reut2-018.sgm\n",
      "Reading file: reut2-019.sgm\n",
      "Reading file: reut2-020.sgm\n",
      "Reading file: reut2-021.sgm\n"
     ]
    }
   ],
   "source": [
    "document_X = {}\n",
    "document_Y = {}\n",
    "\n",
    "document_id=[]\n",
    "document_body=[]\n",
    "document_train_test=[]\n",
    "category=[]\n",
    "document_X=pd.DataFrame()\n",
    "\n",
    "for i in range(sgml_number_of_files):\n",
    "    if i < 10:\n",
    "        seq = '00' + str(i)\n",
    "    else:\n",
    "        seq = '0' + str(i)\n",
    "        \n",
    "    file_name = sgml_file_name_template.replace('NNN', seq)\n",
    "    print('Reading file: %s' % file_name)\n",
    "    \n",
    "    with codecs.open(data_folder + file_name, 'r', encoding='utf-8',\n",
    "                 errors='ignore') as file:\n",
    "        content = BeautifulSoup(file.read().lower())      \n",
    "        for newsline in content('reuters'):\n",
    "                      \n",
    "            # News-line Id\n",
    "            document_id.append(newsline['newid'])\n",
    "            \n",
    "            # News-line text\n",
    "            document_body.append(unescape(strip_tags(str(newsline('text')[0].body)).replace('reuter\\n&#3;', '')))\n",
    "#            document_body.append(newsline.find(\"title\").text)\n",
    "            document_train_test.append(strip_tags(str(newsline.attrs[\"lewissplit\"]) ))\n",
    "            #for c in newsline.find(\"topics\").findAll('d'):\n",
    "                #category.append(c.text)\n",
    "            category.append([item.text for item in newsline.find(\"topics\").findAll('d')])\n",
    "            \n",
    "document_X['document_id'] = document_id\n",
    "document_X['document_body'] = document_body\n",
    "#document_X['document_body'] = document_X['document_body'].str.lower()\n",
    "document_X['document_body'] = document_X['document_body'].apply(cleanHtml)\n",
    "document_X['document_body'] = document_X['document_body'].apply(cleanPunc)\n",
    "document_X['document_body'] = document_X['document_body'].apply(keepAlpha)\n",
    "document_X['document_body'] = document_X['document_body'].apply(removeStopWords)\n",
    "document_X['train_test_label'] = document_train_test\n",
    "document_X['category'] = category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X=document_X[(document_X['train_test_label']=='train') & (document_X['category'].astype(str)!='[]') & (document_X['document_body'].astype(str)!='None')]['document_body'].values.tolist()\n",
    "test_X=document_X[(document_X['train_test_label']=='test') & (document_X['category'].astype(str)!='[]')& (document_X['document_body'].astype(str)!='None')]['document_body'].values.tolist()\n",
    "\n",
    "train_Y=document_X[(document_X['train_test_label']=='train') & (document_X['category'].astype(str)!='[]')& (document_X['document_body'].astype(str)!='None')]['category'].values.tolist()\n",
    "test_Y=document_X[(document_X['train_test_label']=='test') & (document_X['category'].astype(str)!='[]')& (document_X['document_body'].astype(str)!='None')]['category'].values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1,3), norm='l2')\n",
    "vectorizer.fit(train_X)\n",
    "vectorizer.fit(test_X)\n",
    "x_train = vectorizer.transform(train_X)\n",
    "x_test = vectorizer.transform(test_X)\n",
    "\n",
    "\n",
    "# Transform multilabel labels\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "train_labels = mlb.fit_transform(train_Y)\n",
    "test_labels = mlb.transform(test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7068"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLkNN()"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skmultilearn.adapt import  \n",
    "classifier_new = MLkNN(k=10)\n",
    "# train\n",
    "classifier_new.fit(x_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.7686703096539163\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "predictions_new = classifier_new.predict(x_test)\n",
    "\n",
    "# accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy = \",accuracy_score(test_labels,predictions_new))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
